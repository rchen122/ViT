{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf8d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa13d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "\tdef __init__(self, img_size, patch_size, in_channel, embed_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.img_size = img_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.in_channel = in_channel\n",
    "\t\tself.embed_size = embed_size\n",
    "\n",
    "\t\tassert img_size % patch_size == 0\n",
    "\t\tself.num_patches = (img_size // patch_size)**2\n",
    "\t\tself.conv1 = nn.Conv2d(in_channel, self.embed_size, self.patch_size, self.patch_size)\n",
    "\t\tself.cls_token = nn.Parameter(torch.zeros(1, 1, embed_size))\n",
    "\t\tself.pos_embedding = nn.Parameter(torch.zeros(1, self.num_patches+1, embed_size))\n",
    "\t\t\n",
    "\t\tnn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "\t\tnn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\t\t# nn.init.kaiming_normal_(self.proj.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "\tdef forward(self, x): #need to convert [B, 3, 32, 32] to [B, num_patches + 1, embed_size]\n",
    "\t\tx = self.conv1(x) # [B, embed_size, 8, 8]\n",
    "\t\tx = x.flatten(2) #[B, embed_size, 64]\n",
    "\t\tx = x.transpose(1, 2) # [B, 64, embed_size]\n",
    "\n",
    "\t\t#add clk token\n",
    "\t\tB = x.shape[0]\n",
    "\t\tcls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\t\tx = torch.cat((cls_tokens, x), dim=1) # [B, 65, embed_size]\n",
    "\t\tx = x + self.pos_embedding\n",
    "\t\treturn x\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b125aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 65, 256])\n"
     ]
    }
   ],
   "source": [
    "model = PatchEmbedding(img_size=32, patch_size=4, in_channel=3, embed_size=256)\n",
    "demo = torch.randn(8, 3, 32, 32)\n",
    "out = model(demo)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4871a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, num_heads, embed_size, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tassert embed_size % num_heads == 0\n",
    "\n",
    "\t\tself.head_dim = embed_size // num_heads\n",
    "\t\tself.qkv_project = nn.Linear(embed_size, 3 * embed_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.fc = nn.Linear(embed_size, embed_size)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, N, D = x.shape\n",
    "\t\td_k = self.head_dim\n",
    "\t\tqkv = self.qkv_project(x) #[B, N, D*3]\n",
    "\t\tqkv = qkv.reshape(B, N, 3, self.num_heads, d_k) # [B, N, 3, H, d_k]\n",
    "\t\tqkv = qkv.permute(2, 0, 3, 1, 4) # [3, B, H, N, d_k]\n",
    "\t\tQ, K, V = qkv[0], qkv[1], qkv[2] #[B, H, N, d_k]\n",
    "\n",
    "\t\t#Compute Attention Score\n",
    "\t\t# Attention(Q, K, V) = Softmax(QK^T / sqrt(query_size)) * V\n",
    "\t\tscores = (Q @ K.transpose(2, 3)) / (d_k ** 0.5) #[B, H, N, N]\n",
    "\t\tscores = F.softmax(scores, dim = -1)\n",
    "\t\tscores = self.dropout(scores)\n",
    "\t\tcontext = scores @ V # [B, H, N, d_k] ? \n",
    "\t\tcontext = context.transpose(1, 2) # [B, N, H, d_k]\n",
    "\t\tcontext = context.reshape(B, N, D)\n",
    "\n",
    "\t\tout = self.fc(context)\n",
    "\t\treturn out\n",
    "\n",
    "#i dont even know how to test this do i just pray it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fda4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\tdef __init__(self, embed_size, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.norm1 = nn.LayerNorm(embed_size)\n",
    "\t\tself.attn = MultiHeadAttention(num_heads=8, embed_size=embed_size, dropout=dropout)\n",
    "\t\tself.norm2 = nn.LayerNorm(embed_size)\n",
    "\t\tself.MLP = nn.Sequential(\n",
    "\t\t\tnn.Linear(embed_size, embed_size*4), # SECOND ONE CAN BE CHANGED\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(dropout),\n",
    "\t\t\tnn.Linear(embed_size*4, embed_size),\n",
    "\t\t\tnn.Dropout(dropout)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.attn(self.norm1(x))\n",
    "\t\tx = x + self.MLP(self.norm2(x))\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2742d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\tdef __init__(self, num_class, img_size, patch_size, in_channel, embed_size, num_heads, depth, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.img_size = img_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.in_channel = in_channel\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\t\tself.patch_embedding = PatchEmbedding(img_size, patch_size, in_channel, embed_size)\n",
    "\t\tself.layers = nn.ModuleList([\n",
    "\t\t\tTransformerEncoder(embed_size, dropout) for _ in range(depth)\n",
    "\t\t])\n",
    "\t\tself.mlp_head = nn.Sequential(\n",
    "\t\t\tnn.Linear(embed_size, embed_size*4), # ?\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(dropout),\n",
    "\t\t\tnn.Linear(embed_size*4, num_class)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.patch_embedding(x)\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x)\n",
    "\t\tcls_token_out = x[:, 0] #[B, embed_size]\n",
    "\t\tlogits = self.mlp_head(cls_token_out)\n",
    "\t\treturn logits\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a420aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/100], Loss: 3.8603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     42\u001b[0m epochs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, train_loader, num_class, img_size, patch_size, in_channel, embed_size, num_heads, depth, epochs, dropout)\u001b[0m\n\u001b[0;32m     16\u001b[0m \tloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m \toptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 18\u001b[0m \trunning_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_loop(model, train_loader, num_class, img_size, patch_size, in_channel, embed_size, num_heads, depth, epochs, dropout=0.1):\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tloss_fn = torch.nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "\tmodel.train()\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trunning_loss = 0.0\n",
    "\t\t\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\t\toutput = model(images)\n",
    "\t\t\tloss = loss_fn(output, labels)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\tavg_loss = running_loss / len(train_loader)\n",
    "\t\tprint(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "num_class = 100\n",
    "img_size = 32\n",
    "patch_size = 4\n",
    "in_channel = 3\n",
    "embed_size = 256\n",
    "num_heads=8\n",
    "depth=8\n",
    "dropout=0.1\n",
    "\n",
    "model = ViT(num_class=num_class, img_size=img_size, patch_size=patch_size, in_channel=in_channel, embed_size=embed_size, num_heads=num_heads, depth=depth, dropout=0.1)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "\ttorchvision.transforms.RandomHorizontalFlip(),\n",
    "\ttorchvision.transforms.RandomCrop(32, padding=4),\n",
    "\ttorchvision.transforms.ToTensor(),\n",
    "\ttorchvision.transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))  # CIFAR-100 stats\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='data/', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "epochs= 100\n",
    "train_loop(model, train_loader, num_class, img_size, patch_size, in_channel, embed_size, num_heads, depth, epochs, dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
