{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf8d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa13d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "\tdef __init__(self, img_size, patch_size, in_channel, embed_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.img_size = img_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.in_channel = in_channel\n",
    "\t\tself.embed_size = embed_size\n",
    "\n",
    "\t\tassert img_size % patch_size == 0\n",
    "\t\tself.num_patches = (img_size // patch_size)**2\n",
    "\t\tself.conv1 = nn.Conv2d(in_channel, self.embed_size, self.patch_size, self.patch_size)\n",
    "\t\tself.cls_token = nn.Parameter(torch.zeros(1, 1, embed_size))\n",
    "\t\tself.pos_embedding = nn.Parameter(torch.zeros(1, self.num_patches+1, embed_size))\n",
    "\t\t\n",
    "\t\tnn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\t\tnn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\t\tnn.init.kaiming_normal_(self.proj.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "\tdef forward(self, x): #need to convert [B, 3, 32, 32] to [B, num_patches + 1, embed_size]\n",
    "\t\tx = self.conv1(x) # [B, embed_size, 8, 8]\n",
    "\t\tx = x.flatten(2) #[B, embed_size, 64]\n",
    "\t\tx = x.transpose(1, 2) # [B, 64, embed_size]\n",
    "\n",
    "\t\t#add clk token\n",
    "\t\tB = x.shape[0]\n",
    "\t\tcls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\t\tx = torch.cat((cls_tokens, x), dim=1) # [B, 65, embed_size]\n",
    "\t\tx = x + self.pos_embedding\n",
    "\t\treturn x\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b125aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 65, 256])\n"
     ]
    }
   ],
   "source": [
    "model = PatchEmbedding(img_size=32, patch_size=4, in_channel=3, embed_size=256)\n",
    "demo = torch.randn(8, 3, 32, 32)\n",
    "out = model(demo)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, num_heads, embed_size, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tassert embed_size % num_heads == 0\n",
    "\n",
    "\t\tself.head_dim = embed_size // num_heads\n",
    "\t\tself.qkv_project = nn.Linear(embed_size, 3 * embed_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.fc = nn.Linear(embed_size, embed_size)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, N, D = x.shape\n",
    "\t\td_k = self.head_dim\n",
    "\t\tqkv = self.qkv_project(x) #[B, N, D*3]\n",
    "\t\tqkv = qkv.reshape(B, N, 3, self.num_heads, d_k) # [B, N, 3, H, d_k]\n",
    "\t\tqkv = qkv.permute(2, 0, 3, 1, 4) # [3, B, H, N, d_k]\n",
    "\t\tQ, K, V = qkv[0], qkv[1], qkv[2] #[B, H, N, d_k]\n",
    "\n",
    "\t\t#Compute Attention Score\n",
    "\t\t# Attention(Q, K, V) = Softmax(QK^T / sqrt(query_size)) * V\n",
    "\t\tscores = (Q @ K.transpose(2, 3)) / (d_k ** 0.5) #[B, H, N, N]\n",
    "\t\tscores = F.softmax(scores, dim = -1)\n",
    "\t\tscores = self.dropout(scores)\n",
    "\t\tcontext = scores @ V # [B, H, N, d_k] ? \n",
    "\t\tcontext = context.transpose(1, 2) # [B, N, H, d_k]\n",
    "\t\tcontext = context.reshape(B, N, D)\n",
    "\n",
    "\t\tout = self.fc(context)\n",
    "\t\treturn out\n",
    "\n",
    "#i dont even know how to test this do i just pray it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\tdef __init__(self, embed_size, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.norm1 = nn.LayerNorm(embed_size)\n",
    "\t\tself.attn = MultiHeadAttention(num_heads=8, embed_size=embed_size, dropout=dropout)\n",
    "\t\tself.norm2 = nn.LayerNorm(embed_size)\n",
    "\t\tself.MLP = nn.Sequential(\n",
    "\t\t\tnn.Linear(embed_size, embed_size*4), # SECOND ONE CAN BE CHANGED\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(dropout),\n",
    "\t\t\tnn.Linear(embed_size*4, embed_size),\n",
    "\t\t\tnn.Dropout(dropout)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.attn(self.norm1(x))\n",
    "\t\tx = x + self.MLP(self.norm2(x))\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\tdef __init__(self, num_class, img_size, patch_size, in_channel, embed_size, num_heads, depth, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.img_size = img_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.in_channel = in_channel\n",
    "\t\tself.embed_size = embed_size\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\t\tself.patch_embedding = PatchEmbedding(img_size, patch_size, in_channel, embed_size)\n",
    "\t\tself.layers = nn.ModuleList([\n",
    "\t\t\tTransformerEncoder(embed_size, dropout) for _ in range(depth)\n",
    "\t\t])\n",
    "\t\tself.mlp_head = nn.Sequential(\n",
    "\t\t\tnn.Linear(embed_size, embed_size*4), # ?\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(dropout),\n",
    "\t\t\tnn.Linear(embed_size*4, num_class)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.patch_embedding(x)\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x)\n",
    "\t\tcls_token_out = x[:, 0] #[B, embed_size]\n",
    "\t\tlogits = self.mlp_head(cls_token_out)\n",
    "\t\treturn logits\n",
    "\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
